Week 2:
  Created the GitHub repository and Trello board for the group. We
  discussed what project we would do, and ended up with topic 1:
  X-o-Bot. Using my experience in web design, I created a basic
  prototype of a front end webpage in which a user can type in their
  query and it sends off a request to a server.

Week 3:
  We defined the project more precisely, coming to the agreement that
  we'll make a bot that functions similarly to the student center at
  UNSW. The user will type in their question, something like "What
  courses should I do this semester?", and the app will respond with
  an appropriate response. We also delegated tasks for both the proposal
  writing and the project itself. I appointed myself to do the front
  end for both the website and the mobile app.

Week 4:
  Created a prototype for the front end webpage. This wasn't too
  difficult, but it's a very necessary part as the main purpose of this
  app is for users to have a very convenient way to ask questions to the
  bot. I also created a small basic flask server for the purpose of
  testing sending and receiving responses to and from a server.

Week 5:
  Researched into the chatbot more. In order to train the chatbot, we
  would have to create a lot of training data, with many inputs and
  their corresponding outputs to make the bot learn how to correctly
  respond. This does not seem feasible in the amount of time that we
  have for the project, so our next idea was to use a "preprocessing"\
  function, which can extract the key words out of the question, such
  as course and zid, and then pass that to the chatbot.

Week 6:
  Worked on the preprocess function, still having trouble linking it up
  to the chatbot. The chatbot accepts a string input, but the preprocess
  function returns a tuple, so we need to figure out a consistent way
  of extracting the relevant words but leaving the question intact.

Week 7:
  Did more research into the inner workings of the chatbot. The machine
  learning method used by the chatterbot is naive Bayesian
  classification, which is a fairly simplistic but fast method, and is
  a popular method for text classification. For making the chatbot more
  intelligent, I will have to write more test data, but I would also
  have to create a filter, which will be used to more accurately grab
  key words like course codes and be able to process them
  appropriately. As we will mostly likely be heavily using filters for
  this project, I may end up having the bot return templates, where we
  can the add in the relevant data, such as course information.

Week 8:
  It turns out that writing a lot of individual statements is not working
  out, as there are way too many specific responses that I'd need to
  cover. So I'll either have to keep working through it, or use a
  different language parser, one more geared towards sentiment analysis,
  which can check what kind of question they are asking, and then find
  key words which I can use to query the database and then build a
  response.

Week 9:
  I've figured out that the best way to do the question answering
  functionality is to use a more barebones toolkit. Fittingly,
  I've found the NLTK: the Natural Language Toolkit. While this is
  much less fancy than the Chatterbot library we were using previously,
  it does give us a lot more flexibility in terms of how we can
  interpret what the user is saying. I'm going to need to
  do a lot of reading to figure out how to build this chatbot
  from the ground up however

Week 10:
  I completely overhauled the way that the server interprets the
  messages sent by the client. Using the Python NLTK, I have made
  the server extract key words from the statement that the user
  inputs, and then compares it against the database that my
  other group members have made. Currently it only compares the
  course title and the course code, and returns data in the form
  of a 2d array, however, formatted as a string. To interpret
  the data, I split the first layer array by a pipe "|", and the
  second layer array by a comma ",". In future weeks I will
  extend this functionality to include other tables, and I
  will also make the natural language processing more
  sophisticated, to allow for actual question asking and
  answering, rather than the search engine it is now.
